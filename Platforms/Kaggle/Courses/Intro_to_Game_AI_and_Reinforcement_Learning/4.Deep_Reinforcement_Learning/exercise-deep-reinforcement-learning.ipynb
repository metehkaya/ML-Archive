{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nIn the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines` package to train an agent to beat a random opponent.  In this exercise, you will check your understanding and tinker with the code to deepen your intuition.","metadata":{}},{"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:43.796833Z","iopub.execute_input":"2021-08-11T15:37:43.797148Z","iopub.status.idle":"2021-08-11T15:37:43.910003Z","shell.execute_reply.started":"2021-08-11T15:37:43.797081Z","shell.execute_reply":"2021-08-11T15:37:43.908994Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 1) Set the architecture\n\nIn the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes: one for each column in the game board.\n\nSay now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n\n- Option A: 2 nodes (number of game players)\n- Option B: 16 nodes (number of game pieces that each player starts with)\n- Option C: 4672 nodes (number of possible moves)\n- Option D: 64 nodes (number of squares on the game board)\n\nUse your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`.","metadata":{}},{"cell_type":"code","source":"# Fill in the blank\nbest_option = 'C'\n\n# Check your answer\nq_1.check()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:43.911736Z","iopub.execute_input":"2021-08-11T15:37:43.912039Z","iopub.status.idle":"2021-08-11T15:37:43.921657Z","shell.execute_reply.started":"2021-08-11T15:37:43.912011Z","shell.execute_reply":"2021-08-11T15:37:43.920748Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: If we use a similar network as in the tutorial, the network should output a probability for each possible move.","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> If we use a similar network as in the tutorial, the network should output a probability for each possible move."},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you solution code\nq_1.solution()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:43.923228Z","iopub.execute_input":"2021-08-11T15:37:43.923555Z","iopub.status.idle":"2021-08-11T15:37:43.933032Z","shell.execute_reply.started":"2021-08-11T15:37:43.923524Z","shell.execute_reply":"2021-08-11T15:37:43.932236Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\nbest_option = 'C'\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\nbest_option = 'C'\n```"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2) Decide reward\n\nIn the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https://bit.ly/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n\nTo play this game in Google Search, click on the **[Play]** button at [this link](https://www.google.com/search?q=minesweeper).  \n\n<center>\n<img src=\"https://i.imgur.com/WzoEfKY.png\" width=50%><br/>\n</center>\n\nWith each move, one of the following is true:\n- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n- The agent detonates a mine and loses the game.\n\nHow might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n\nAfter you have decided on your answer, run the code cell below to get credit for completing this question.","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nq_2.solution()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:43.934053Z","iopub.execute_input":"2021-08-11T15:37:43.934453Z","iopub.status.idle":"2021-08-11T15:37:43.945727Z","shell.execute_reply.started":"2021-08-11T15:37:43.934415Z","shell.execute_reply":"2021-08-11T15:37:43.944587Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2_DecideReward\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n- If agent wins the game in that move, it gets a reward of `+1`.\n- Else if the agent selects an invalid move, it gets a reward of `-10`.\n- Else if it detonates a mine, it gets a reward of `-1`.\n- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n\nTo check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n- If agent wins the game in that move, it gets a reward of `+1`.\n- Else if the agent selects an invalid move, it gets a reward of `-10`.\n- Else if it detonates a mine, it gets a reward of `-1`.\n- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n\nTo check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."},"metadata":{}}]},{"cell_type":"markdown","source":"### 3) (Optional) Amend the code\n\nIn this next part of the exercise, you will amend the code from the tutorial to experiment with creating your own agents!  There are a lot of hyperparameters involved with specifying a reinforcement learning agent, and you'll have a chance to amend them, to see how performance is affected.\n\nFirst, we'll need to make sure that your Kaggle Notebook is set up to run the code.  Begin by looking at the \"Settings\" menu to the right of your notebook.  Your menu will look like one of the following:\n\n<center>\n<img src=\"https://i.imgur.com/kR1az0y.png\" width=100%><br/>\n</center>\n\nIf your \"Internet\" setting appears as a \"Requires phone verification\" link, click on this link.  This will bring you to a new window; then, follow the instructions to verify your account.  After following this step, your \"Internet\" setting will appear \"Off\", as in the example to the right.\n\nOnce your \"Internet\" setting appears as \"Off\", click to turn it on.  You'll see a pop-up window that you'll need to \"Accept\" in order to complete the process and have the setting switched to \"On\".  Once the Internet is turned \"On\", you're ready to proceed!\n\n<center>\n<img src=\"https://i.imgur.com/gOVh6Aa.png\" width=100%><br/>\n</center>\n\nBegin by running the code cell below. ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n!pip install 'tensorflow==1.15.0'\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy\n\nclass ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n    \n# Create ConnectFour environment\nenv = ConnectFourGym(agent2=\"random\")\n\n# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])\n\n# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:43.947261Z","iopub.execute_input":"2021-08-11T15:37:43.947564Z","iopub.status.idle":"2021-08-11T15:40:46.496483Z","shell.execute_reply.started":"2021-08-11T15:37:43.947536Z","shell.execute_reply":"2021-08-11T15:40:46.495056Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting tensorflow==1.15.0\n  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n\u001b[K     |████████████████████████████████| 412.3 MB 18 kB/s s eta 0:00:01     |█████████████▋                  | 176.0 MB 52.0 MB/s eta 0:00:05     |█████████████████▍              | 223.9 MB 43.5 MB/s eta 0:00:05     |█████████████████████████████▏  | 376.0 MB 55.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.1.2)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.15.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.36.2)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.19.5)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.32.0)\nCollecting tensorflow-estimator==1.15.1\n  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n\u001b[K     |████████████████████████████████| 503 kB 50.4 MB/s eta 0:00:01\n\u001b[?25hCollecting gast==0.2.2\n  Downloading gast-0.2.2.tar.gz (10 kB)\nCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.1.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.12.1)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (3.17.3)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.12.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15.0) (3.3.0)\nCollecting keras-applications>=1.0.8\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s  eta 0:00:01\n\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 41.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (2.0.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (49.6.0.post20210108)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\nBuilding wheels for collected packages: gast\n  Building wheel for gast (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=3b3db1efbfbb091e5419515847eb8b34f5522a59ba7c7e1c8423ae645d99c06e\n  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\nSuccessfully built gast\nInstalling collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, astor, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.4.0\n    Uninstalling tensorflow-estimator-2.4.0:\n      Successfully uninstalled tensorflow-estimator-2.4.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.4.1\n    Uninstalling tensorboard-2.4.1:\n      Successfully uninstalled tensorboard-2.4.1\n  Attempting uninstall: gast\n    Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.4.1\n    Uninstalling tensorflow-2.4.1:\n      Successfully uninstalled tensorflow-2.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\ntensorflow-cloud 0.1.13 requires tensorboard>=2.3.0, but you have tensorboard 1.15.0 which is incompatible.\npytorch-lightning 1.3.8 requires tensorboard!=2.5.0,>=2.2.0, but you have tensorboard 1.15.0 which is incompatible.\u001b[0m\nSuccessfully installed astor-0.8.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nLoading environment football failed: No module named 'gfootball'\nGet:1 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5385 B]\nGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \nGet:3 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] \nHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease                        \nGet:5 http://packages.cloud.google.com/apt cloud-sdk InRelease [6739 B]        \nGet:6 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [454 B]\nGet:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nGet:8 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [199 kB]\nGet:9 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [180 kB]\nGet:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\nGet:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]   \nGet:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2263 kB]\nGet:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2195 kB]\nGet:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\nGet:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1420 kB]\nGet:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\nGet:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\nGet:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2699 kB]\nFetched 10.3 MB in 2s (4185 kB/s)                       \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nzlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\npython3-dev is already the newest version (3.6.7-1~18.04).\npython3-dev set to manually installed.\nThe following additional packages will be installed:\n  autotools-dev cmake-data file ibverbs-providers libfabric1 libhwloc-dev\n  libhwloc-plugins libhwloc5 libibverbs-dev libltdl-dev libmagic-mgc libmagic1\n  libnuma-dev libopenmpi2 libpsm-infinipath1 librdmacm1 libtool\n  ocl-icd-libopencl1 openmpi-bin openmpi-common\nSuggested packages:\n  cmake-doc ninja-build libhwloc-contrib-plugins libtool-doc openmpi-doc\n  autoconf automaken gfortran | fortran95-compiler gcj-jdk opencl-icd gfortran\nThe following NEW packages will be installed:\n  autotools-dev file ibverbs-providers libfabric1 libhwloc-dev\n  libhwloc-plugins libhwloc5 libibverbs-dev libltdl-dev libmagic-mgc libmagic1\n  libnuma-dev libopenmpi-dev libopenmpi2 libpsm-infinipath1 librdmacm1 libtool\n  ocl-icd-libopencl1 openmpi-bin openmpi-common\nThe following packages will be upgraded:\n  cmake cmake-data\n2 upgraded, 20 newly installed, 0 to remove and 23 not upgraded.\nNeed to get 9505 kB of archives.\nAfter this operation, 25.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\nGet:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cmake amd64 3.10.2-1ubuntu2.18.04.2 [3152 kB]\nGet:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cmake-data all 3.10.2-1ubuntu2.18.04.2 [1332 kB]\nGet:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ibverbs-providers amd64 17.1-1ubuntu0.2 [160 kB]\nGet:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-5 [174 kB]\nGet:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 librdmacm1 amd64 17.1-1ubuntu0.2 [56.1 kB]\nGet:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfabric1 amd64 1.5.3-1 [302 kB]\nGet:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl-dev amd64 2.4.6-2 [162 kB]\nGet:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\nGet:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc5 amd64 1.11.9-1 [105 kB]\nGet:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 ocl-icd-libopencl1 amd64 2.2.11-1ubuntu1 [30.3 kB]\nGet:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-plugins amd64 1.11.9-1 [12.5 kB]\nGet:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi2 amd64 2.1.1-8 [2056 kB]\nGet:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-common all 2.1.1-8 [140 kB]\nGet:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-bin amd64 2.1.1-8 [88.2 kB]\nGet:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnuma-dev amd64 2.0.11-2.1ubuntu0.1 [32.3 kB]\nGet:20 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-dev amd64 1.11.9-1 [167 kB]\nGet:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibverbs-dev amd64 17.1-1ubuntu0.2 [103 kB]\nGet:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi-dev amd64 2.1.1-8 [925 kB]\nFetched 9505 kB in 2s (4383 kB/s)        \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libmagic-mgc.\n(Reading database ... 96799 files and directories currently installed.)\nPreparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\nUnpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\nSelecting previously unselected package libmagic1:amd64.\nPreparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\nUnpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\nSelecting previously unselected package file.\nPreparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\nUnpacking file (1:5.32-2ubuntu0.4) ...\nSelecting previously unselected package autotools-dev.\nPreparing to unpack .../03-autotools-dev_20180224.1_all.deb ...\nUnpacking autotools-dev (20180224.1) ...\nPreparing to unpack .../04-cmake_3.10.2-1ubuntu2.18.04.2_amd64.deb ...\nUnpacking cmake (3.10.2-1ubuntu2.18.04.2) over (3.10.2-1ubuntu2.18.04.1) ...\nPreparing to unpack .../05-cmake-data_3.10.2-1ubuntu2.18.04.2_all.deb ...\nRemove cmake-data for emacs25\nremove/cmake-data: Purging byte-compiled files for emacs25\nUnpacking cmake-data (3.10.2-1ubuntu2.18.04.2) over (3.10.2-1ubuntu2.18.04.1) ...\nSelecting previously unselected package ibverbs-providers:amd64.\nPreparing to unpack .../06-ibverbs-providers_17.1-1ubuntu0.2_amd64.deb ...\nUnpacking ibverbs-providers:amd64 (17.1-1ubuntu0.2) ...\nSelecting previously unselected package libpsm-infinipath1.\nPreparing to unpack .../07-libpsm-infinipath1_3.3+20.604758e7-5_amd64.deb ...\nUnpacking libpsm-infinipath1 (3.3+20.604758e7-5) ...\nSelecting previously unselected package librdmacm1:amd64.\nPreparing to unpack .../08-librdmacm1_17.1-1ubuntu0.2_amd64.deb ...\nUnpacking librdmacm1:amd64 (17.1-1ubuntu0.2) ...\nSelecting previously unselected package libfabric1.\nPreparing to unpack .../09-libfabric1_1.5.3-1_amd64.deb ...\nUnpacking libfabric1 (1.5.3-1) ...\nSelecting previously unselected package libltdl-dev:amd64.\nPreparing to unpack .../10-libltdl-dev_2.4.6-2_amd64.deb ...\nUnpacking libltdl-dev:amd64 (2.4.6-2) ...\nSelecting previously unselected package libtool.\nPreparing to unpack .../11-libtool_2.4.6-2_all.deb ...\nUnpacking libtool (2.4.6-2) ...\nSelecting previously unselected package libhwloc5:amd64.\nPreparing to unpack .../12-libhwloc5_1.11.9-1_amd64.deb ...\nUnpacking libhwloc5:amd64 (1.11.9-1) ...\nSelecting previously unselected package ocl-icd-libopencl1:amd64.\nPreparing to unpack .../13-ocl-icd-libopencl1_2.2.11-1ubuntu1_amd64.deb ...\nUnpacking ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\nSelecting previously unselected package libhwloc-plugins.\nPreparing to unpack .../14-libhwloc-plugins_1.11.9-1_amd64.deb ...\nUnpacking libhwloc-plugins (1.11.9-1) ...\nSelecting previously unselected package libopenmpi2:amd64.\nPreparing to unpack .../15-libopenmpi2_2.1.1-8_amd64.deb ...\nUnpacking libopenmpi2:amd64 (2.1.1-8) ...\nSelecting previously unselected package openmpi-common.\nPreparing to unpack .../16-openmpi-common_2.1.1-8_all.deb ...\nUnpacking openmpi-common (2.1.1-8) ...\nSelecting previously unselected package openmpi-bin.\nPreparing to unpack .../17-openmpi-bin_2.1.1-8_amd64.deb ...\nUnpacking openmpi-bin (2.1.1-8) ...\nSelecting previously unselected package libnuma-dev:amd64.\nPreparing to unpack .../18-libnuma-dev_2.0.11-2.1ubuntu0.1_amd64.deb ...\nUnpacking libnuma-dev:amd64 (2.0.11-2.1ubuntu0.1) ...\nSelecting previously unselected package libhwloc-dev:amd64.\nPreparing to unpack .../19-libhwloc-dev_1.11.9-1_amd64.deb ...\nUnpacking libhwloc-dev:amd64 (1.11.9-1) ...\nSelecting previously unselected package libibverbs-dev:amd64.\nPreparing to unpack .../20-libibverbs-dev_17.1-1ubuntu0.2_amd64.deb ...\nUnpacking libibverbs-dev:amd64 (17.1-1ubuntu0.2) ...\nSelecting previously unselected package libopenmpi-dev.\nPreparing to unpack .../21-libopenmpi-dev_2.1.1-8_amd64.deb ...\nUnpacking libopenmpi-dev (2.1.1-8) ...\nSetting up libltdl-dev:amd64 (2.4.6-2) ...\nSetting up cmake-data (3.10.2-1ubuntu2.18.04.2) ...\nInstall cmake-data for emacs25\ninstall/cmake-data: Byte-compiling for emacs25\nemacs25: /opt/conda/lib/libtiff.so.5: no version information available (required by emacs25)\nSetting up librdmacm1:amd64 (17.1-1ubuntu0.2) ...\nSetting up libhwloc5:amd64 (1.11.9-1) ...\nSetting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\nSetting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\nSetting up libpsm-infinipath1 (3.3+20.604758e7-5) ...\nupdate-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\nSetting up openmpi-common (2.1.1-8) ...\nSetting up autotools-dev (20180224.1) ...\nSetting up ibverbs-providers:amd64 (17.1-1ubuntu0.2) ...\nSetting up libnuma-dev:amd64 (2.0.11-2.1ubuntu0.1) ...\nSetting up ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\nSetting up cmake (3.10.2-1ubuntu2.18.04.2) ...\nSetting up libibverbs-dev:amd64 (17.1-1ubuntu0.2) ...\nSetting up libfabric1 (1.5.3-1) ...\nSetting up libhwloc-dev:amd64 (1.11.9-1) ...\nSetting up file (1:5.32-2ubuntu0.4) ...\nSetting up libhwloc-plugins (1.11.9-1) ...\nSetting up libopenmpi2:amd64 (2.1.1-8) ...\nSetting up libtool (2.4.6-2) ...\nSetting up libopenmpi-dev (2.1.1-8) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/mpi (mpi) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\nSetting up openmpi-bin (2.1.1-8) ...\nupdate-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\nProcessing triggers for libc-bin (2.27-3ubuntu1.4) ...\nRequirement already satisfied: stable-baselines[mpi]==2.9.0 in /opt/conda/lib/python3.7/site-packages (2.9.0)\nRequirement already satisfied: gym[atari,classic_control]>=0.10.9 in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (0.18.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (1.0.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (3.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (1.19.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (1.2.4)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (4.5.2.54)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (1.6.3)\nRequirement already satisfied: cloudpickle>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from stable-baselines[mpi]==2.9.0) (1.6.0)\nCollecting mpi4py\n  Using cached mpi4py-3.0.3.tar.gz (1.4 MB)\nRequirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.5.15)\nRequirement already satisfied: Pillow<=8.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (8.2.0)\nRequirement already satisfied: atari-py~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (0.2.9)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.15.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (2.8.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (0.10.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines[mpi]==2.9.0) (2021.1)\nBuilding wheels for collected packages: mpi4py\n  Building wheel for mpi4py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp37-cp37m-linux_x86_64.whl size=2088498 sha256=980e2f8cfa44d79297c94d277272cce03c00105ad5ab9ba642cbc99647ae881e\n  Stored in directory: /root/.cache/pip/wheels/da/37/ee/8d5c9166a378bb0b661bf4257b8e1ef8d79d879b931534fb98\nSuccessfully built mpi4py\nInstalling collected packages: mpi4py\nSuccessfully installed mpi4py-3.0.3\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial.","metadata":{}},{"cell_type":"code","source":"# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0)\n\n# Train agent\nmodel.learn(total_timesteps=100000)\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:40:46.498821Z","iopub.execute_input":"2021-08-11T15:40:46.499317Z","iopub.status.idle":"2021-08-11T15:51:41.919236Z","shell.execute_reply.started":"2021-08-11T15:40:46.499245Z","shell.execute_reply":"2021-08-11T15:51:41.918140Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n\nOnce you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n  - Advantage Actor-Critic (A2C),\n  - or Actor-Critic with Experience Replay (ACER),\n  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n  - Trust Region Policy Optimization (TRPO).\n- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!","metadata":{}},{"cell_type":"markdown","source":"# Congratulations!\n\nYou have completed the course, and it's time to put your new skills to work!  \n\nThe next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n\nYou can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n\nAs we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161477) to chat with other Learners.*","metadata":{}}]}